{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6c5a821-2de3-4039-a42b-4727a265ed88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Load the Data from CSV file into the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0fa3523d-8332-4a77-b92d-b33165926bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.csv(\"/FileStore/tables/Order.csv\", header=True, inferSchema=True, sep =',')\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5ccbe28-a690-40ae-9f6f-16519dc02c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Display the Data using the Databricks Specific Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abf16b10-8786-4c45-a137-f1c8a18435d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f9023789-7a65-4d69-94b6-ce8985a46fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Data Type in the pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "295786bd-0920-4b1a-9c18-4ea0f313060e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data type                             Value assigned in Python                   API to instantiate\n",
    "#ByteType                                 \tint                                     DataTypes.ByteType\n",
    "#ShortType                            \t\tint                                     DataTypes.ShortType\n",
    "#IntegerType                         \t\tint                   \t\t\t\t\tDataTypes.IntegerType\n",
    "#LongType                           \t\tint                 \t\t\t\t\tDataTypes.LongType\n",
    "#FloatType                           \t\tfloat              \t\t\t\t\t\tDataTypes.FloatType\n",
    "#DoubleType                          \t\tfloat                  \t\t\t\t\tDataTypes.DoubleType\n",
    "#StringType                          \t\tstr             \t\t\t\t\t\tDataTypes.StringType\n",
    "#BooleanType                          \t\tbool                \t\t\t\t\tDataTypes.BooleanType\n",
    "#DecimalType                          \t\tdecimal.Decimal         \t\t\t\tDecimalType\n",
    "#BinaryType \t\t\t\t\t\t\t\tbytearray \t\t\t\t\t\t\t\tBinaryType()\n",
    "#TimestampType \t\t\t\t\t\t\t\tdatetime.datetime \t\t\t\t\t\tTimestampType()\n",
    "#DateType \t\t\t\t\t\t\t\t\tdatetime.date \t\t\t\t\t\t\tDateType()\n",
    "#ArrayType \t\t\t\t\t\t\t\t\tList, tuple, or array \t\t\t\t\tArrayType(dataType, [nullable])\n",
    "#MapType \t\t\t\t\t\t\t\t\tdict \t\t\t\t\t\t\t\t\tMapType(keyType, valueType, [nullable])\n",
    "#StructType \t\t\t\t\t\t\t\tList or tuple \t\t\t\t\t\t\tStructType([fields])\n",
    "#StructField \t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\tStructField(name, dataType, [nullable])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ecc2e8ce-923b-4a44-8a48-7ee27ea76f1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Define schema progarmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76db805f-a77c-4487-a8e3-29c1669cd5ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define schema progarmatically\n",
    "from pyspark.sql.types import *\n",
    "orderSchema = StructType([StructField(\"Region\", StringType() ,True)\n",
    ",StructField(\"Country\", StringType() ,True)\n",
    ",StructField(\"ItemType\", StringType() ,True)\n",
    ",StructField(\"SalesChannel\", StringType() ,True)\n",
    ",StructField(\"OrderPriority\", StringType() ,True)\n",
    ",StructField(\"OrderID\", IntegerType() ,True)\n",
    ",StructField(\"UnitsSold\", IntegerType() ,True)\n",
    ",StructField(\"UnitPrice\", DoubleType() ,True)\n",
    ",StructField(\"UnitCost\", DoubleType() ,True)\n",
    ",StructField(\"TotalRevenue\", DoubleType() ,True)\n",
    ",StructField(\"TotalCost\", DoubleType() ,True)\n",
    ",StructField(\"TotalProfit\", DoubleType() ,True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.load(\"/FileStore/tables/Order.csv\",format=\"csv\", header=True, schema=orderSchema)\n",
    "df.printSchema()\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "782dbd21-1033-41e7-afb9-957b89e2a1b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Define schema Declaratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f77e6c4c-af9f-4a7a-a6aa-09ccb556a103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "orderSchema = 'Region String ,Country String ,ItemType String ,SalesChannel String ,OrderPriority String ,OrderID Integer ,UnitsSold Integer ,UnitPrice Double ,UnitCost Double ,TotalRevenue Double ,TotalCost Double ,TotalProfit Double'\n",
    "                          \n",
    "df = spark.read.load(\"/FileStore/tables/Order.csv\",format=\"csv\", header=True, schema=orderSchema)\n",
    "df.printSchema()\n",
    "#df.schema\n",
    "#display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f139a35-68c2-4d43-a396-69c5943ab1d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc3fdb99-5c5c-413a-9313-05974feb3a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60db82ec-16d3-4dda-9ddd-a6873c117185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c6debfe-8948-4f81-8953-a0d8e73c94c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Columns in Spark Dataframe \n",
    " in Scala\n",
    "$\"myColumn\"\n",
    "'myColumn\n",
    "\n",
    "Columns are just expressions.<br>\n",
    "Columns and transformations of those columns compile to the same logical plan as\n",
    "parsed expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a9c47f4-b3b7-4c78-99fd-33da5a13414b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76c47a20-f044-4c57-b7f7-3b43f88e4671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To get list of columns\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e31c08a-a731-47d7-b1ea-ccfb8eb7b1c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To get the first row\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37e9b735-280a-4914-ae49-7615ba07d8ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Create Dataframe and Rows Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc078575-8be1-42ea-8713-af2673ac0a7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row1 = Row(\"Ram\", None, 1, True)\n",
    "\n",
    "# To access the specific attribute within the row use following:\n",
    "#parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "row1[0]\n",
    "\n",
    "# Create a dataframe\n",
    "row1 = Row(\"Ram\", None, 1, True)\n",
    "myManualSchema = 'Name string, address string not null, id integer, exists string'\n",
    "manDf = spark.createDataFrame([row1], myManualSchema)\n",
    "manDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d2798f5-d40e-4bc0-ae8f-1287026edbb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Select and SelectExpr (Select Expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34eb7650-3d82-410f-bf83-05f97a77377e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\"Region\", \"Country\").show(2)\n",
    "\n",
    "#select using variety of ways\n",
    "df.select(\n",
    "expr(\"Country\"),\n",
    "col(\"Region\"),\n",
    "column(\"ItemType\"),\n",
    "df.OrderID)\\\n",
    ".show(2)\n",
    "\n",
    "# Select using the alias\n",
    "df.select(\n",
    "expr(\"Country as NewCountry\"),\n",
    "col(\"Region\").alias('New Region'),\n",
    "column(\"ItemType\"),\n",
    "df.OrderID)\\\n",
    ".show(2)\n",
    "\n",
    "#Use selectexpr\n",
    "\n",
    "df.selectExpr(\"Country as NewCountry\", \"Region\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eee4c3d6-9ab4-473b-91f3-b00b495d81f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Converting to Spark Types (Literals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7600247a-0989-4045-81e7-8190ecebb4c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"NumberOne\")).show(2)\n",
    "df.select(expr(\"*\"), lit(True).alias(\"MyResult\")).show(2)\n",
    "df.select(expr(\"*\"), lit('Constant').alias(\"AddedColumn\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24494bd5-0285-4a13-8836-f27f93b397b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Add column to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1d4b63f5-5591-465e-a177-15307e81b418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "newdf = df.withColumn(\"NewColumn\", lit(1)).show(2)\n",
    "newdf = df.withColumn(\"withinCountry\", expr(\"Country == 'India'\"))\\\n",
    "\n",
    "newdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67dc2624-d65e-40de-a2dd-8d0f38df6629",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b5725c26-48ef-44bd-add7-47af66bdfd5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "newdf = df.withColumnRenamed(\"Country\", \"NewCountry\")\n",
    "newdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4de5f5f4-3aa5-45f2-adfc-e84b776ec807",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use escape character when column name contains spaces (`)\n",
    "newdf =df.withColumnRenamed(\"Country\",\"New Column Name\")\n",
    "newdf.select(\"`New Column Name`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "20505d03-919b-4fa2-9dbf-4f91a65910b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    " #Removing Columns\n",
    " \n",
    " Use comma to remove multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03706bb5-551d-4a11-a197-b98007de7099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.drop(\"Country\").columns\n",
    "df.drop(\"Country\", \"Region\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79e3d754-c433-4a91-8c0f-04dbb0405383",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Change Column Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a53e2118-1b26-462b-b74b-2e08cabc04d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn(\"UnitsSoldNew\", col(\"UnitsSold\").cast(\"double\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de0171e8-1072-47b4-af74-14438c6e374b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Filter Rows\n",
    "To filter rows, we create an expression that evaluates to true or false. You then filter out the rows\n",
    "with an expression that is equal to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25192187-a4e8-4c62-b7c5-e7b84c3b2101",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(col(\"UnitsSold\") < 1000).show(2)\n",
    "df.where(\"UnitsSold < 1000\").show(2)\n",
    "df.filter(df.UnitsSold < 1000).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "776a5780-e03a-4b6f-a00c-a4228a08185b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Unique Rows\n",
    "extract the unique or distinct values in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d40ea18-edf0-45f2-846d-a39a985bd088",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"Country\", \"Region\").distinct().count()\n",
    "df.select(\"Country\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4fe78aae-7edb-4a95-b0be-82be34c7726f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Union\n",
    "DataFramess. To union two DataFrames, you must be sure that they have the same schema and\n",
    "number of columns; otherwise, the union will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7eafdc83-11f6-404e-b33d-4cb9fb988a46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.filter(\"Country = 'Libya'\")\n",
    "df2 = df.filter(\"Country = 'Canada'\")\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "22f29d28-f296-441b-b1d1-cf35b6376d74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Sorting\n",
    "There are two equivalent operations to do this sort\n",
    "and orderBy that work the exact same way. They accept both column expressions and strings as\n",
    "well as multiple columns. The default is to sort in ascending order: <br>\n",
    "You need to use the asc and desc functions if operating\n",
    "on a column. These allow you to specify the order in which a given column should be sorted: <br>\n",
    "\n",
    "use asc_nulls_first, desc_nulls_first, asc_nulls_last, or\n",
    "desc_nulls_last to specify where you would like your null values to appear in an ordered\n",
    "DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5daaf19e-9596-434c-95bc-57f9e50829a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sort(\"Country\").show(5)\n",
    "df.orderBy(\"Country\", \"UnitsSold\").show(5)\n",
    "df.orderBy(col(\"ItemType\"), col(\"UnitPrice\")).show(5)\n",
    "\n",
    "#asc and desc function\n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"Country desc\")).show(2)\n",
    "df.orderBy(col(\"Country\").desc(), col(\"UnitPrice\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "437e0356-693b-46f3-b947-cd8fe95e3318",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Limit\n",
    "you\n",
    "might want just the top ten of some DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a3262d2-29b3-490c-a340-9830a41928df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(5).show()\n",
    "df.orderBy(expr(\"Country desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df86999e-7596-4614-b5ef-870ceeb78449",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Repartition and Coalesce\n",
    "\n",
    "Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This\n",
    "means that you should typically only repartition when the future number of partitions is greater\n",
    "than your current number of partitions or when you are looking to partition by a set of columns: <br> <br>\n",
    "Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. This\n",
    "operation will shuffle your data into five partitions based on the destination country name, and\n",
    "then coalesce them (without a full shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32a178b3-ee1f-4bef-8cc5-78c50fc5cd07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "df.repartition(5)\n",
    "\n",
    "#If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column:\n",
    "df.repartition(5, col(\"Country\"))\n",
    "\n",
    "\n",
    "df.repartition(5, col(\"Country\")).coalesce(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccf333c9-0fa1-40ec-949c-66ae8846591a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use collect carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf07f77e-46f8-4823-8830-371d477cb3f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7812ee84-5073-4c9e-bda0-634d0d77d6ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Describe to get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3795825-d78e-4965-a140-ae1da357edbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82dad903-b1dd-43b5-83b4-d72441278bc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#String functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9515925e-46a9-4a98-b40e-3673bfa650ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "#Translate the first letter of each word to upper case in the sentence.\n",
    "df.select(initcap(col(\"Country\"))).show()\n",
    "df.select(lower(col(\"Country\"))).show()\n",
    "df.select(upper(col(\"Country\"))).show()\n",
    "\n",
    "#To concat \n",
    "display(df.select(concat(col(\"Region\"), col(\"Country\"))))\n",
    "\n",
    "#To concat with separater\n",
    "display(df.select(concat_ws('|',col(\"Region\"), col(\"Country\"))))\n",
    "\n",
    "\n",
    "#instr(str, substr) - Returns the (1-based) index of the first occurrence of substr in str\n",
    "display(df.select(instr(col(\"Region\"),\"Mi\")))\n",
    "\n",
    "#length(expr) - Returns the character length of string data or number of bytes of binary data\n",
    "display(df.select(length(col(\"Region\"))))\n",
    "\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)\n",
    "\n",
    "# Regular Expression\n",
    "#from pyspark.sql.functions import regexp_replace\n",
    "#regex_string = \"Hello|WHITE|RED|GREEN|BLUE\"\n",
    "#df.select(regexp_replace(col(\"Country\"), regex_string, \"COLOR\").alias(\"color_clean\"),col(\"Description\")).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c0b71d6-0d27-4ecc-98b8-efb2e323ea0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Date Handling in Spark\n",
    "\n",
    "Spark will not throw an error if it cannot parse the date; rather, it will just return null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "90208d91-db34-4193-9532-00c266b67e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "\n",
    "display(dateDF)\n",
    "\n",
    "# Add Subtract dates\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n",
    "\n",
    "#Days and Month difference between dates\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "\n",
    "dateDF.select(\n",
    "to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)\n",
    "\n",
    "#incorrect date format\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)\n",
    "\n",
    "#Give date format\n",
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.show()\n",
    "\n",
    "#Handle timestamp to date  format casting\n",
    "from pyspark.sql.functions import to_timestamp, year, month, dayofmonth, hour, minute, second\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()\n",
    "\n",
    "#get year using the year function with date and timepstamp\n",
    "cleanDateDF.select(year(to_timestamp(col(\"date\"), dateFormat))).show()\n",
    "\n",
    "#get month using the month function with date and timepstamp\n",
    "cleanDateDF.select(month(to_timestamp(col(\"date\"), dateFormat))).show()\n",
    "\n",
    "#get dayofmonth using the dayofmonth function with date and timepstamp\n",
    "cleanDateDF.select(dayofmonth(to_timestamp(col(\"date\"), dateFormat))).show()\n",
    "\n",
    "\n",
    "#get hour using the hour function with date and timepstamp\n",
    "cleanDateDF.select(hour(to_timestamp(col(\"date\"), dateFormat))).show()\n",
    "\n",
    "\n",
    "#get minute using the minute function with date and timepstamp\n",
    "cleanDateDF.select(minute(to_timestamp(col(\"date\"), dateFormat))).show()\n",
    "\n",
    "\n",
    "#get second using the second function with date and timepstamp\n",
    "cleanDateDF.select(second(to_timestamp(col(\"date\"), dateFormat))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06fb3356-9582-4e1b-938e-a65c2be18c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Coalesce\n",
    "\n",
    "Spark includes a function to allow you to select the first non-null value from a set of columns by\n",
    "using the coalesce function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c752fb6-244b-41bd-aaac-a423fefdb2c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6552aa96-8413-404c-b463-2d99c85b1c64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#ifnull, nullIf, nvl, and nvl2\n",
    "\n",
    "There are several other SQL functions that you can use to achieve similar things. ifnull allows\n",
    "you to select the second value if the first is null, and defaults to the first. Alternatively, you could\n",
    "use nullif, which returns null if the two values are equal or else returns the second if they are\n",
    "not. nvl returns the second value if the first is null, but defaults to the first. Finally, nvl2 returns\n",
    "the second value if the first is not null; otherwise, it will return the last specified value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f19cf9e4-bdeb-442a-afe1-3fff11daa824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/*SELECT\n",
    "ifnull(null, 'return_value'),\n",
    "nullif('value', 'value'),\n",
    "nvl(null, 'return_value'),\n",
    "nvl2('not_null', 'return_value', \"else_value\")\n",
    "FROM dfTable LIMIT 1*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3fee3b80-7fce-4310-8f30-e92d6c406370",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Drop\n",
    "The simplest function is drop, which removes rows that contain nulls. The default is to drop any\n",
    "row in which any value is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f92361c4-9c97-449d-8c9b-3413debeca79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.na.drop()\n",
    "\n",
    "#Specifying \"any\" as an argument drops a row if any of the values are null. Using “all” drops the row only if all values are null or NaN for that row:\n",
    "df.na.drop(\"any\")\n",
    "df.na.drop(\"all\")\n",
    "\n",
    "#We can also apply this to certain sets of columns by passing in an array of columns:\n",
    "df.na.drop(\"all\", subset=[\"Country\", \"Region\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7826f63-a358-4008-8f51-7aeb6bb6c66a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Fill \n",
    "\n",
    "Using the fill function, you can fill one or more columns with a set of values. This can be done\n",
    "by specifying a map—that is a particular value and a set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4efe12b-5e4f-4a59-9706-5ab6d79863d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#For example, to fill all null values in columns of type String, you might specify the following:\n",
    "df.na.fill(\"All Null values become this string\")\n",
    "df.na.fill(\"all\", subset=[\"Country\", \"Region\"])\n",
    "\n",
    "fill_cols_vals = {\"UnitsSold\": 5, \"Region\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c47203ab-2218-4ebc-8a0d-1fc28eb51726",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Split column to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d3fba7f-ed41-4178-8cac-c4e059fba761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Region\"), \" \")).show(2)\n",
    "\n",
    "df.select(split(col(\"Region\"), \" \").alias(\"Region_Array\"))\\\n",
    ".selectExpr(\"Region_Array[0]\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10a5614f-2736-4be1-97c7-bac077022cea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Array Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "074a202e-549d-4bb2-95b7-a22674bf2d87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(col(\"Region\"), \" \"))).show(2) # shows 5 and 3\n",
    "\n",
    "#Array contains\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Region\"), \" \"), \"North\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "19126e3d-86d3-4d38-9aac-2882ebe1ffc9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Where NotNul function\n",
    "from pyspark.sql.functions import *\n",
    "df2 = df.select(\"Region\", \"Country\", \"UnitsSold\").where(df.Country == \"Libya\")\n",
    "df2.show()\n",
    "df2 = df.select(\"Region\", \"Country\", df[\"UnitsSold\"]+1).where(df.Country == \"Libya\")\n",
    "df2.show()\n",
    "df2.show(5, truncate=False)\n",
    "df2 = df.select(\"Region\", \"Country\", df[\"UnitsSold\"].isNotNull()).where(df.Country == \"Libya\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "327bbc11-c70e-43e5-9fec-1a3599f70d0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df2= df.filter((df.Country  == \"Libya\") |  (df.Country  == \"Japan\"))\n",
    "#df2.show()\n",
    "df.createOrReplaceTempView(\"Order\")\n",
    "df2 = spark.sql (\"select * from Order where Country in ('Libya', 'Japan')\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "406a481d-3901-4838-ab6d-3f1a71475917",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#explode\n",
    "\n",
    "The explode function takes a column that consists of arrays and creates one row (with the rest of\n",
    "the values duplicated) per value in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "80ad912f-8414-41ed-a4fd-f1c2b10cebc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df.withColumn(\"splitted\", split(col(\"Region\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Region\", \"Country\", \"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "78bd3fa3-72f3-4543-b35b-3b9397873ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Maps\n",
    "Maps are created by using the map function and key-value pairs of columns. You then can select\n",
    "them just like you might select from an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6270faa1-be3f-4c06-9807-cc19aa5952d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Region\"), col(\"Country\")).alias(\"complex_map\"))\\\n",
    ".show(2)\n",
    "\n",
    "#query map\n",
    "#df.select(map(col(\"Region\"), col(\"Country\")).alias(\"complex_map\"))\\\n",
    "#.selectExpr(\"complex_map['Middle']\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1c3893d-db87-4977-8e69-7f17fba9aff6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4ea6afb-a992-4839-a869-b6faa9fa459b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "df.select(count(\"Region\")).show() \n",
    "\n",
    "#countDistinct\n",
    "df.select(countDistinct(\"Region\")).show()\n",
    "#-- in SQL\n",
    "#SELECT COUNT(DISTINCT *) FROM DFTABLE\n",
    "\n",
    "#first and last\n",
    "#You can get the first and last values from a DataFrame by using these two obviously named functions.\n",
    "\n",
    "# in Python\n",
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"Region\"), last(\"Region\")).show()\n",
    "\n",
    "#min and max\n",
    "#To extract the minimum and maximum values from a DataFrame, use the min and max functions:\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"UnitsSold\"), max(\"UnitsSold\")).show()\n",
    "\n",
    "#sum\n",
    "#Another simple task is to add all the values in a row using the sum function:\n",
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"UnitsSold\")).show()\n",
    "\n",
    "#sumDistinct\n",
    "#In addition to summing a total, you also can sum a distinct set of values by using the sumDistinct function:\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"UnitsSold\")).show() \n",
    "\n",
    "#avg\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "df.select(avg(\"UnitsSold\")).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a357d208-ea3d-4e49-a812-43e7f7971ee5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Grouping\n",
    "\n",
    " this returns another DataFrame and is lazily performed. We do this grouping in two phases. First we specify the column(s) on which we would like to\n",
    "group, and then we specify the aggregation(s). The first step returns a\n",
    "RelationalGroupedDataset, and the second step returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "17311dc4-10df-4a44-a924-cfc5c120636e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"Region\", \"Country\").count().show()\n",
    "\n",
    "#-- in SQL\n",
    "#SELECT count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId\n",
    "\n",
    "# in Python\n",
    "#Rather than passing that function as an expression into a select statement, we specify it as within agg. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like alias a column after transforming it for later use in your data flow:\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"Region\").agg(\n",
    "count(\"UnitsSold\").alias(\"quan\"),\n",
    "expr(\"count(UnitsSold)\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91bde60b-b57c-48a0-ae22-35bb00ce0aab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0eeb2ebf-7b81-4254-9b5b-917957a2d3b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression:\n",
    "\n",
    "person = spark.createDataFrame([\n",
    "(0, \"Bill Chambers\", 0, [100]),\n",
    "(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "(2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    ".toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    ".toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "(500, \"Vice President\"),\n",
    "(250, \"PMC Member\"),\n",
    "(100, \"Contributor\")])\\\n",
    ".toDF(\"id\", \"status\")\n",
    "\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "person.join(graduateProgram, joinExpression).show()\n",
    "\n",
    "\n",
    "# specify the join type\n",
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()\n",
    "\n",
    "#Outer Joins\n",
    "#Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. If there is no equivalent row in either the left or right DataFrame, Spark will insert null:\n",
    "\n",
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()\n",
    "\n",
    "#Left Outer Joins\n",
    "#Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert null:\n",
    "\n",
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n",
    "\n",
    "\n",
    "#Right Outer Joins\n",
    "#Right outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the right DataFrame as well as any rows in the left DataFrame that have a match in the right DataFrame. If there is no equivalent row in the left DataFrame, Spark will insert null:\n",
    "\n",
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()\n",
    "\n",
    "#Natural Joins\n",
    "#Natural joins make implicit guesses at the columns on which you would like to join. It finds matching columns and returns the results. Left, right, and outer natural joins are all supported.\n",
    "\n",
    "#-- in SQL\n",
    "#SELECT * FROM graduateProgram NATURAL JOIN person\n",
    "\n",
    "#Cross (Cartesian) Joins\n",
    "#The last of our joins are cross-joins or cartesian products. Cross-joins in simplest terms are inner joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. If you have 1,000 rows in each DataFrame, the cross\u0002join of these will result in 1,000,000 (1,000 x 1,000) rows. For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword:\n",
    "\n",
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n",
    "\n",
    "#-- in SQL\n",
    "#SELECT * FROM graduateProgram CROSS JOIN person\n",
    "#ON graduateProgram.id = person.graduate_program\n",
    "\n",
    "#If you truly intend to have a cross-join, you can call that out explicitly:\n",
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff3e6ffc-970f-4754-9598-2a73a86cfa01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Write to dataframe <br>\n",
    "\n",
    "append::           Appends the output files to the list of files that already exist at that location<br>\n",
    "overwrite::         Will completely overwrite any data that already exists there<br>\n",
    "errorIfExists::     Throws an error and fails the write if data or files already exist at the specified location<br>\n",
    "ignore::            If data or files exist at the location, do nothing with the current DataFrame<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e7ac4b4-afd2-41dd-8580-6d474572fc17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe.write.format(\"csv\")\n",
    ".option(\"mode\", \"OVERWRITE\")\n",
    ".option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    ".option(\"path\", \"path/to/file(s)\")\n",
    ".save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "50492587-dde3-48f0-b13e-da44f55db2a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Parquet Format\n",
    "\n",
    "Parquet Files\n",
    "Parquet is an open source column-oriented data store that provides a variety of storage\n",
    "optimizations, especially for analytics workloads. It provides columnar compression, which\n",
    "saves storage space and allows for reading individual columns instead of entire files. It is a file\n",
    "format that works exceptionally well with Apache Spark and is in fact the default file format. We\n",
    "recommend writing data out to Parquet for long-term storage because reading from a Parquet file\n",
    "will always be more efficient than JSON or CSV. Another advantage of Parquet is that it\n",
    "supports complex types. This means that if your column is an array (which would fail with a\n",
    "CSV file, for example), map, or struct, you’ll still be able to read and write that file without\n",
    "issue. Here’s how to specify Parquet as the read format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7069aabe-c4da-427e-8da9-268536726e34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"parquet\")\\\n",
    ".load(\"/data/flight-data/parquet/2010-summary.parquet\").show(5)\n",
    "\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    ".save(\"/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2dd3c498-4230-40d5-80ce-6ef979dc3358",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9a8f17f-e382-4cc0-bb77-46d0b8c4630b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbDataFrame.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6b024dc2-5fcb-495b-b00e-f12788e0ba30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Filter Functions\n",
    "df2= df.filter(df.Country != \"Libya\")\n",
    "df2.show()\n",
    "\n",
    "#Multiple Condition\n",
    "df2= df.filter((df.Country  == \"Libya\") |  (df.Country  == \"Japan\"))\n",
    "df2.show()\n",
    "\n",
    "#List of values filter\n",
    "li=[\"Libya\",\"Japan\"]\n",
    "df.filter(df.Country.isin(li)).show()\n",
    "\n",
    "#Like Filter\n",
    "df.filter(df.Country.like(\"%J%\")).show()\n",
    "\n",
    "#Regular Expression Filter\n",
    "df.filter(df.Country.rlike(\"(?i)^*L$\")).show()\n",
    "\n",
    "#Assuming that one of the column in dataframe is Array then You can run filter using the below code\n",
    "#from pyspark.sql.functions import array_contains\n",
    "#df.filter(array_contains(df.phoneNumbers,\"123\")).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de9f5f42-0c69-4e9e-b47b-947c1678c4d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 =df.withColumn(\"UnitSoldNew\",df.UnitsSold+1)\n",
    "df3 = df2.select(\"UnitSoldNew\")\n",
    "df4 = df3.drop()\n",
    "df4.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be3a41e1-60a3-4612-a350-ad7b25ef7343",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add Columns\n",
    "df2 =df.withColumn(\"UnitSoldNew\",df.UnitsSold+1)\n",
    "df2.show()\n",
    "\n",
    "# Column Rename\n",
    "df2 = df2.withColumnRenamed(\"UnitSoldNew\",\"UnitSoldRenamed\")\n",
    "df2.show()\n",
    "\n",
    "#Column Drop\n",
    "df2 = df2.drop(\"UnitSoldRenamed\")\n",
    "#or\n",
    "df2 = df2.drop(df2.UnitsSold)\n",
    "df2.show()\n",
    "\n",
    "#Change DataType of Column\n",
    "df2 = df.withColumn(\"UnitsSold\",df.UnitsSold.cast(\"Double\")).show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d427294b-9478-45ea-9a44-d02702f3eff0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Show Function Variation\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "df.show()\n",
    "df.show(10)\n",
    "df.show(truncate=False)\n",
    "\n",
    "#Usually, collect() is used to retrieve the action output when you have very small result set and calling collect() on an RDD/DataFrame with a bigger result set causes out of memory as it returns the entire dataset (from all workers) to the driver hence we should avoid calling collect() on a larger dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9532ca83-b3c0-489b-9d8c-8d5078dca74e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#String Functions\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df2 =df.select(\"Region\",\"Country\",F.when(df.UnitsSold > 2000, 1).otherwise(0))\n",
    "df2.show()\n",
    "\n",
    "#check isin\n",
    "df2 = df[df.Country.isin(\"Libiya\" ,\"Japan\")]\n",
    "df2.show()\n",
    "\n",
    "#Like\n",
    "df2 = df.select(\"Region\",\"Country\", df.Country.like(\"L\" ))\n",
    "df2.show()\n",
    "\n",
    "#StartsWith\n",
    "df2 = df.select(\"Region\",\"Country\", df.Country.startswith(\"L\"))\n",
    "df2.show()\n",
    " \n",
    "df2 = df.select(\"Region\",\"Country\", df.Country.endswith(\"L\"))\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e72b1fd8-758f-44c9-8cda-cc35aadcc780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Missing Data\n",
    "\n",
    "#Replace null with given value for the list of columns\n",
    "df2 = df.fillna(value=0,subset=[\"Country\"])\n",
    "df2.show()\n",
    "\n",
    "#Another way to replace\n",
    "df2= df.na.fill(value=0, subset=[\"Country\"])\n",
    "df2.show()\n",
    "\n",
    "#Replace null for all columns\n",
    "df2 = df.na.fill(0)\n",
    "df2.show()\n",
    "\n",
    "\n",
    "#drop all null rows\n",
    "df2 = df.na.drop()\n",
    "df2.show() \n",
    "\n",
    "#replace value\n",
    "df2 = df.na.replace(10, 20)\n",
    "\n",
    " .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f86e489-cbc5-4273-a6f3-970b51edfbfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Distinct / Remove Duplicate\n",
    "df2 = df.dropDuplicates()  \n",
    "dropDisDF = df.dropDuplicates([\"Country\",\"UnitsSold\"])\n",
    "df2 =df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7111aaaa-31fd-4419-8685-aa3598ce7c3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Sorting and Order By\n",
    "df.sort(\"Country\",\"Region\").show(truncate=False)\n",
    "df.sort(df.Country,df.Region).show(truncate=False)\n",
    "\n",
    "#Ascending / descending\n",
    "df.sort(df.Country.asc(),df.Region.desc()).show(truncate=False)\n",
    "\n",
    "#Order bY function is also works same as sort\n",
    "df.orderBy(df.Country.asc(),df.Region.desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b34d3f3a-92e6-4f00-ba1c-31cc9be44567",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum,avg,max\n",
    "#group by\n",
    "df.groupBy(df.Country).count().show()\n",
    "\n",
    "#Multiple column grouping\n",
    "df.groupBy(df.Country, df.Region).sum(\"UnitsSold\").show()\n",
    "\n",
    "#Multiple aggregation\n",
    "df.groupBy(df.Country, df.Region).sum(\"UnitsSold\",\"UnitCost\").show()\n",
    "\n",
    "#Multiple Different Type of aggregation\n",
    "df.printSchema()\n",
    "df.groupBy(df.Country, df.Region).sum(\"UnitsSold\").where(col(\"sum(UnitsSold)\")>1000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "039af8a5-a69c-4b3d-b09a-939a15fd0c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c0c96ddb-a50f-49e5-b93d-0611b5c20481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--CREATE DATABASE sample\n",
    "\n",
    "select * from OrderTable18Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ade8254d-c0fe-4182-8883-7bfcc0d3ba8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#CREATE DATABASE sample\n",
    "#df.write.mode(\"overwrite\").saveAsTable(\"OrderTable18Dec\")\n",
    "dbutils.fs(\"ls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "40a1d635-fbdb-4f4d-9e95-1cafd5276e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df.write.mode(\"overwrite\").saveAsTable(\"OrdeYabler\")\n",
    "# dataframe.write.mode(\"overwrite\").option(\"path\",\"<your-storage-path>\").saveAsTable(\"<example-table>\")   Unmanaged Overwrite\n",
    "df.write.mode(\"overwrite\").partitionBy(\"Country\").saveAsTable(\"OrderPartition18Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "09ad041d-1abe-4b20-bacc-dc1abca2e0a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "rm /user/hive/warehouse/ordeyabler/part-00000-dd01437f-1f50-4cc6-91dd-f7b1b194476e-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1aa4efeb-3c02-4376-be14-9bc75051a53b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--REFRESH TABLE OrdeYabler\n",
    "--select * from OrdeYabler\n",
    "--Drop table OrdeYabler\n",
    "--CRUD\n",
    "-- SELECT * FROM OrderPartition18Dec WHERE OrderID=686800706\n",
    "-- DESCRIBE history OrderPartition18Dec\n",
    "\n",
    "SELECT * FROM OrderPartition18Dec VERSION AS OF 1 WHERE OrderID=686800706\n",
    "UPDATE  VERSION AS OF 0 SET Country = 'Libya' WHERE  OrderID=686800706\n",
    "\n",
    "\n",
    " --SELECT * FROM order_delta WHERE OrderID=686800706\n",
    " -- DELETE FROM OrderPartition18Dec WHERE OrderID=686800706\n",
    "-- Confirm the user's data was deleted\n",
    "/*SELECT * FROM order_delta WHERE OrderID=686800706\n",
    "\n",
    "INSERT INTO order_delta\n",
    "SELECT * FROM order_delta VERSION AS OF 0\n",
    "WHERE OrderID=686800706\n",
    "\n",
    "UPDATE  VERSION AS OF 0 SET Country = 'Libya' WHERE  OrderID=686800706\n",
    "\n",
    "\n",
    "-- Vacuum deletes all files no longer needed by the current version of the table.\n",
    "VACUUM  VERSION AS OF 0\n",
    "\n",
    "CACHE SELECT * FROM  VERSION AS OF 0\n",
    "OPTIMIZE  VERSION AS OF 0 ZORDER BY Country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "69177469-d917-46e0-b4bc-ee64885dd73d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"order_delta5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1b5c1ef-c482-400f-98ae-194958d19def",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from order_delta5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab6338d0-0c0b-4c41-9ae5-fb55404a8e25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE loans_delta2\n",
    "USING delta\n",
    "AS SELECT * FROM parquet.'/tmp/delta_demo/loans_parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c397c31-8718-4a4b-80e5-d6c364cc4171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Use CONVERT TO DELTA to convert Parquet files to Delta Lake format in place\n",
    "CONVERT TO DELTA parquet. /tmp/delta_demo/loans_parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a7da6f2-cae9-4ef5-a497-287674b6af7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "DESCRIBE HISTORY order_delta\n",
    "--View the Delta Lake transaction log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3317a66-338e-4053-98a9-e474bafb53b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Use Schema Evolution to add new columns to schema\n",
    "new_data.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"loans_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a4c4929b-e639-4116-ba75-5f199e1c37ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#verison\n",
    "#DESCRIBE HISTORY loans_delta\n",
    "spark.sql(\"SELECT * FROM order_delta VERSION AS OF 0\").show(3)\n",
    "#spark.sql(\"SELECT COUNT(*) FROM loans_delta VERSION AS OF 0\").show()\n",
    "\n",
    "#Rollback\n",
    "%sql \n",
    "#RESTORE loans_delta VERSION AS OF 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f5739c7c-f831-4960-9978-af4313b00796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--CRUD\n",
    "\n",
    "-- describe history order_delta\n",
    "SELECT * FROM order_delta VERSION AS OF 1  WHERE OrderID=686800706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "790db094-e940-4fb2-b8b3-8a2945142b76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--SELECT * FROM order_delta5 WHERE OrderID=686800706\n",
    "--DELETE FROM order_delta5 WHERE OrderID=686800706\n",
    "SELECT * FROM order_delta5 VERSION AS OF 3 WHERE OrderID=686800706\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3d7102d-7da8-434a-827c-e7c0d1c2f702",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "--CRUD\n",
    "SELECT * FROM order_delta WHERE OrderID=686800706\n",
    "DELETE FROM order_delta WHERE OrderID=6868007060\n",
    "-- Confirm the user's data was deleted\n",
    "SELECT * FROM order_delta WHERE OrderID=686800706\n",
    "\n",
    "INSERT INTO order_delta\n",
    "SELECT * FROM order_delta VERSION AS OF 0\n",
    "WHERE OrderID=686800706\n",
    "\n",
    "UPDATE  VERSION AS OF 0 SET Country = 'Libya' WHERE  OrderID=686800706\n",
    "\n",
    "\n",
    "-- Vacuum deletes all files no longer needed by the current version of the table.\n",
    "VACUUM  VERSION AS OF 0\n",
    "\n",
    "CACHE SELECT * FROM  VERSION AS OF 0\n",
    "OPTIMIZE  VERSION AS OF 0 ZORDER BY Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "039ad928-2212-451a-8a51-2276bb98bd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create secret in the key vault to store the storage account key\n",
    "# Got to https://community.cloud.databricks.com/?o=9003695902647151#secrets/createScope \n",
    "# provide the key vault URL and resource ID from the KeyValut Properties and give scope name of your choice\n",
    "# azurelibstorageaccountky  = key vault secet name\n",
    "#\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "  source = \"wasbs://myfirstcontainer@azureifystorageaccount.blob.core.windows.net\",\n",
    "  mount_point = \"/mnt\",\n",
    "  extra_configs = {\"fs.azure.account.key.azureifystorageaccount.blob.core.windows.net\":dbutils.secrets.get(scope = \"azurelibscope\", key = \"azurelibstorageaccountky\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec952d3b-9ad2-4873-b0b8-771afb7b117d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "BluePrint",
   "notebookOrigID": 464658254375561,
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

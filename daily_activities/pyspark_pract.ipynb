{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|salary|\n",
      "+----------+---------+------+\n",
      "|      John|      Doe| 50000|\n",
      "|      Jane|    Smith| 60000|\n",
      "|     Alice|  Johnson| 70000|\n",
      "|       Bob|    Brown| 55000|\n",
      "+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    Row(first_name=\"John\", last_name=\"Doe\", salary=50000),\n",
    "    Row(first_name=\"Jane\", last_name=\"Smith\", salary=60000),\n",
    "    Row(first_name=\"Alice\", last_name=\"Johnson\", salary=70000),\n",
    "    Row(first_name=\"Bob\", last_name=\"Brown\", salary=55000)\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of Row objects\n",
    "df = spark.createDataFrame(data)\n",
    "df.select(df[\"first_name\"], df[\"last_name\"], df[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|salary|\n",
      "+----------+---------+------+\n",
      "|      John|      Doe| 50000|\n",
      "|      Jane|    Smith| 60000|\n",
      "|     Alice|  Johnson| 70000|\n",
      "|       Bob|    Brown| 55000|\n",
      "+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|    Smith|\n",
      "|     Alice|  Johnson|\n",
      "|       Bob|    Brown|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"first_name\", \"last_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "result = df.groupBy(\"first_name\").agg(\n",
    "    f.count(\"*\").alias(\"count\"),\n",
    "    f.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    f.max(\"salary\").alias(\"max_salary\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+----------+\n",
      "|first_name|count|avg_salary|max_salary|\n",
      "+----------+-----+----------+----------+\n",
      "|      John|    1|   50000.0|     50000|\n",
      "|      Jane|    1|   60000.0|     60000|\n",
      "|     Alice|    1|   70000.0|     70000|\n",
      "|       Bob|    1|   55000.0|     55000|\n",
      "+----------+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+----------+\n",
      "|first_name|last_name|salary|department|\n",
      "+----------+---------+------+----------+\n",
      "|      John|      Doe| 50000|     sales|\n",
      "|      Jane|    Smith| 60000|     sales|\n",
      "|     Alice|  Johnson| 70000|     sales|\n",
      "|       Bob|    Brown| 55000|     sales|\n",
      "+----------+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "add_column = df.withColumn(\n",
    "    \"department\",\n",
    "    lit(\"sales\")\n",
    ")\n",
    "add_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    Row(emp_id=1, first_name=\"John\", last_name=\"Doe\", salary=50000, department_name=\"Sales\", manager_id=10, manager_name=\"Alice\"),\n",
    "    Row(emp_id=2, first_name=\"Jane\", last_name=\"Smith\", salary=60000, department_name=\"Engineering\", manager_id=20, manager_name=\"Bob\"),\n",
    "    Row(emp_id=3, first_name=\"Alice\", last_name=\"Johnson\", salary=70000, department_name=\"HR\", manager_id=30, manager_name=\"Charlie\"),\n",
    "    Row(emp_id=4, first_name=\"Bob\", last_name=\"Brown\", salary=55000, department_name=\"Sales\", manager_id=10, manager_name=\"Alice\"),\n",
    "    Row(emp_id=5, first_name=\"Eve\", last_name=\"White\", salary=65000, department_name=\"Engineering\", manager_id=20, manager_name=\"Bob\")\n",
    "]\n",
    "\n",
    "# Create DataFrame from the sample data\n",
    "df_emp = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+---------------+----------+------------+\n",
      "|emp_id|first_name|last_name|salary|department_name|manager_id|manager_name|\n",
      "+------+----------+---------+------+---------------+----------+------------+\n",
      "|     1|      John|      Doe| 50000|          Sales|        10|       Alice|\n",
      "|     2|      Jane|    Smith| 60000|    Engineering|        20|         Bob|\n",
      "|     3|     Alice|  Johnson| 70000|             HR|        30|     Charlie|\n",
      "|     4|       Bob|    Brown| 55000|          Sales|        10|       Alice|\n",
      "|     5|       Eve|    White| 65000|    Engineering|        20|         Bob|\n",
      "+------+----------+---------+------+---------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|salary|\n",
      "+----------+---------+------+\n",
      "|      Jane|    Smith| 60000|\n",
      "|       Eve|    White| 65000|\n",
      "+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.select(\"first_name\", \"last_name\", \"salary\")\\\n",
    "    .where((df_emp[\"salary\"] > 50000) & (df_emp[\"department_name\"] == \"Engineering\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+---------------+----------+\n",
      "|emp_id|first_name|last_name|salary|department_name|manager_id|\n",
      "+------+----------+---------+------+---------------+----------+\n",
      "|     1|      John|      Doe| 50000|          Sales|        10|\n",
      "|     2|      Jane|    Smith| 60000|    Engineering|        20|\n",
      "|     3|     Alice|  Johnson| 70000|             HR|        30|\n",
      "|     4|       Bob|    Brown| 55000|          Sales|        10|\n",
      "|     5|       Eve|    White| 65000|    Engineering|        20|\n",
      "+------+----------+---------+------+---------------+----------+\n",
      "\n",
      "+----------+------------+\n",
      "|manager_id|manager_name|\n",
      "+----------+------------+\n",
      "|        10|       Alice|\n",
      "|        20|         Bob|\n",
      "|        30|     Charlie|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_employees = [\n",
    "    Row(emp_id=1, first_name=\"John\", last_name=\"Doe\", salary=50000, department_name=\"Sales\", manager_id=10),\n",
    "    Row(emp_id=2, first_name=\"Jane\", last_name=\"Smith\", salary=60000, department_name=\"Engineering\", manager_id=20),\n",
    "    Row(emp_id=3, first_name=\"Alice\", last_name=\"Johnson\", salary=70000, department_name=\"HR\", manager_id=30),\n",
    "    Row(emp_id=4, first_name=\"Bob\", last_name=\"Brown\", salary=55000, department_name=\"Sales\", manager_id=10),\n",
    "    Row(emp_id=5, first_name=\"Eve\", last_name=\"White\", salary=65000, department_name=\"Engineering\", manager_id=20)\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(data_employees)\n",
    "df_employees.show()\n",
    "\n",
    "data_managers = [\n",
    "    Row(manager_id=10, manager_name=\"Alice\"),\n",
    "    Row(manager_id=20, manager_name=\"Bob\"),\n",
    "    Row(manager_id=30, manager_name=\"Charlie\")\n",
    "]\n",
    "\n",
    "df_managers = spark.createDataFrame(data_managers)\n",
    "df_managers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"c:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\kumav\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e5d5ca65c5a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_joined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_employees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_managers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_employees\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"manager_id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdf_managers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"manager_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_joined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32mc:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\py4j\\clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                 \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                 \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_joined = df_employees.join(df_managers, df_employees[\"manager_id\"] == df_managers[\"manager_id\"], \"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+---------------+----------+----------+------------+\n",
      "|emp_id|first_name|last_name|salary|department_name|manager_id|manager_id|manager_name|\n",
      "+------+----------+---------+------+---------------+----------+----------+------------+\n",
      "|     1|      John|      Doe| 50000|          Sales|        10|        10|       Alice|\n",
      "|     2|      Jane|    Smith| 60000|    Engineering|        20|        20|         Bob|\n",
      "|     3|     Alice|  Johnson| 70000|             HR|        30|        30|     Charlie|\n",
      "|     4|       Bob|    Brown| 55000|          Sales|        10|        10|       Alice|\n",
      "|     5|       Eve|    White| 65000|    Engineering|        20|        20|         Bob|\n",
      "+------+----------+---------+------+---------------+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined = df_employees.join(df_managers, df_employees[\"manager_id\"] == df_managers[\"manager_id\"], \"left\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|department_name|employee_count|\n",
      "+---------------+--------------+\n",
      "|          Sales|             2|\n",
      "|    Engineering|             2|\n",
      "|             HR|             1|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department_count_df = df_employees.groupBy(\"department_name\").agg(\n",
    "    F.count(\"*\").alias(\"employee_count\")\n",
    ")\n",
    "department_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [\n",
    "    (1, \"John\", \"Doe\", 50000),\n",
    "    (2, \"Jane\", \"Smith\", 60000),\n",
    "    (3, \"Alice\", \"Johnson\", 70000),\n",
    "    (4, \"Bob\", \"Brown\", 55000),\n",
    "    (5, \"Eve\", \"White\", 65000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+\n",
      "|emp_id|first_name|last_name|salary|\n",
      "+------+----------+---------+------+\n",
      "|     1|      John|      Doe| 50000|\n",
      "|     2|      Jane|    Smith| 60000|\n",
      "|     3|     Alice|  Johnson| 70000|\n",
      "|     4|       Bob|    Brown| 55000|\n",
      "|     5|       Eve|    White| 65000|\n",
      "+------+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employees = spark.createDataFrame(data, [\"emp_id\", \"first_name\", \"last_name\", \"salary\"])\n",
    "df_employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+\n",
      "|first_name|last_name|salary|\n",
      "+----------+---------+------+\n",
      "|       Eve|    White| 65000|\n",
      "+----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(F.col(\"salary\").desc())\n",
    "\n",
    "df_with_rank = df_employees.withColumn(\"rank\", F.dense_rank().over(window_spec))\n",
    "second_highest = df_with_rank.filter(df_with_rank[\"rank\"] == 2).select(\"first_name\", \"last_name\", \"salary\")\n",
    "\n",
    "second_highest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+---------------+----------+------------+\n",
      "|emp_id|first_name|last_name|salary|department_name|manager_id|manager_name|\n",
      "+------+----------+---------+------+---------------+----------+------------+\n",
      "|     1|      John|      Doe| 50000|          Sales|        10|       Alice|\n",
      "|     2|      Jane|    Smith| 60000|    Engineering|        20|         Bob|\n",
      "|     3|     Alice|  Johnson| 70000|             HR|        30|     Charlie|\n",
      "|     4|       Bob|    Brown| 55000|          Sales|        10|       Alice|\n",
      "|     5|       Eve|    White| 65000|    Engineering|        20|         Bob|\n",
      "+------+----------+---------+------+---------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+---------------+----+\n",
      "|first_name|last_name|salary|department_name|rank|\n",
      "+----------+---------+------+---------------+----+\n",
      "|      Jane|    Smith| 60000|    Engineering|   1|\n",
      "|       Eve|    White| 65000|    Engineering|   2|\n",
      "|     Alice|  Johnson| 70000|             HR|   1|\n",
      "|      John|      Doe| 50000|          Sales|   1|\n",
      "|       Bob|    Brown| 55000|          Sales|   2|\n",
      "+----------+---------+------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_row = Window.partitionBy(\"department_name\").orderBy(\"salary\")\n",
    "\n",
    "df_row_num = df_employees.withColumn(\"rank\", F.row_number().over(window_row))\n",
    "result_df = df_row_num.select(\"first_name\", \"last_name\", \"salary\",\"department_name\", \"rank\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+------+---------------+----------+------------+---------------+\n",
      "|emp_id|first_name|last_name|salary|department_name|manager_id|manager_name|salary_category|\n",
      "+------+----------+---------+------+---------------+----------+------------+---------------+\n",
      "|     1|      John|      Doe| 50000|          Sales|        10|       Alice|           High|\n",
      "|     2|      Jane|    Smith| 60000|    Engineering|        20|         Bob|           High|\n",
      "|     3|     Alice|  Johnson| 70000|             HR|        30|     Charlie|           High|\n",
      "|     4|       Bob|    Brown| 55000|          Sales|        10|       Alice|           High|\n",
      "|     5|       Eve|    White| 65000|    Engineering|        20|         Bob|           High|\n",
      "+------+----------+---------+------+---------------+----------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_case_when = df_employees.withColumn(\n",
    "            \"salary_category\", \n",
    "            F.when(df_employees[\"salary\"] > 10000, \"High\")\n",
    "             .when(df_employees[\"salary\"] == 70000,  \"low\")\n",
    "             .otherwise(\"heavy\")\n",
    "            )\n",
    "df_case_when.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Mahesh:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x6d63cb0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|formatted_date|\n",
      "+--------------+\n",
      "|    2025-01-22|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1).select(F.date_format(F.current_date(), \"yyyy-MM-dd\").alias(\"formatted_date\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|employee_name|manager_name|\n",
      "+-------------+------------+\n",
      "|        Alice|     Charlie|\n",
      "|          Bob|     Charlie|\n",
      "|      Charlie|        null|\n",
      "|        David|         Bob|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", 3),\n",
    "    (2, \"Bob\", 3),\n",
    "    (3, \"Charlie\", None),\n",
    "    (4, \"David\", 2)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"employee_id\", \"employee_name\", \"manager_id\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_employees = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Perform self-join\n",
    "# We join df_employees with itself using aliases to differentiate between the employee and manager sides\n",
    "df_with_manager = df_employees.alias(\"emp\").join(df_employees.alias(\"mgr\"), col(\"emp.manager_id\") == col(\"mgr.employee_id\"),\"left\" )\n",
    "\n",
    "# Select relevant columns (e.g., employee name and manager name)\n",
    "df_with_manager = df_with_manager.select(\n",
    "    col(\"emp.employee_name\").alias(\"employee_name\"),\n",
    "    col(\"mgr.employee_name\").alias(\"manager_name\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_with_manager.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 2), ('world', 1), ('from', 1), ('spark', 3), ('word', 1), ('count', 1), ('program', 1), ('in', 1), ('is', 1), ('awesome', 1)]\n",
      "hello: 2\n",
      "world: 1\n",
      "from: 1\n",
      "spark: 3\n",
      "word: 1\n",
      "count: 1\n",
      "program: 1\n",
      "in: 1\n",
      "is: 1\n",
      "awesome: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import RDD\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"WordCount\").getOrCreate()\n",
    "text_data = [\n",
    "    \"Hello world\",\n",
    "    \"Hello from Spark\",\n",
    "    \"Word count program in Spark\",\n",
    "    \"Spark is awesome\"\n",
    "]\n",
    "\n",
    "# Parallelize the data to create an RDD (Resilient Distributed Dataset)\n",
    "rdd = spark.sparkContext.parallelize(text_data)\n",
    "\n",
    "# Split the lines into words, flatten the list, and map each word to (word, 1)\n",
    "words_rdd = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "               .map(lambda word: (word.lower(), 1))  # Convert words to lowercase to count them uniformly\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_count_rdd = words_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect the results and print them\n",
    "word_counts = word_count_rdd.collect()\n",
    "print(word_counts)\n",
    "\n",
    "# Output the word counts\n",
    "for word, count in word_counts:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[37] at readRDDFromFile at PythonRDD.scala:274\n",
      "PythonRDD[38] at RDD at PythonRDD.scala:53\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text_data = [\n",
    "    \"Hello world\",\n",
    "    \"Hello from Spark\",\n",
    "    \"Word count program in Spark\",\n",
    "    \"Spark is awesome\"\n",
    "]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(text_data)\n",
    "print(rdd)\n",
    "\n",
    "word_d = rdd.flatMap(lambda line: line.split(' ').map(lambda word: (word.lower()), 1))\n",
    "\n",
    "print(word_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pandas 1.0.4 pypi', 'numpy 1.19.5 pypi', 'cherry red']\n",
      "['pandas', '1.0.4', 'pypi']\n",
      "pandas==1.0.4\n",
      "['numpy', '1.19.5', 'pypi']\n",
      "numpy==1.19.5\n",
      "['cherry', 'red']\n",
      "cherry==red\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "pandas 1.0.4 pypi\n",
    "numpy 1.19.5 pypi\n",
    "cherry red\n",
    "\"\"\"\n",
    "\n",
    "# Split the input text into lines\n",
    "lines = input_text.strip().split(\"\\n\")\n",
    "print(lines)\n",
    "\n",
    "# Loop through each line\n",
    "for line in lines:\n",
    "    # Split each line into parts\n",
    "    parts = line.split()\n",
    "    print(parts)\n",
    "    \n",
    "    # If the line has at least two elements (name and version), print them\n",
    "    if len(parts) >= 2:\n",
    "        print(f\"{parts[0]}=={parts[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4f9bff991c51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mrdd_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [(1,2), (2,4), (3,6)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrdd_flatMap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [1, 2, 2, 4, 3, 6]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# sc = SparkContext(\"local\", \"Map Example\")\n",
    "\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd_map = rdd.map(lambda x: (x, x*2))  # [(1,2), (2,4), (3,6)]\n",
    "rdd_flatMap = rdd.flatMap(lambda x: (x, x*2))  # [1, 2, 2, 4, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

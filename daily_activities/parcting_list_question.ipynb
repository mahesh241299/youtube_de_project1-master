{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahesh\n"
     ]
    }
   ],
   "source": [
    "print(\"mahesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumav\\Desktop\\Spark_Project\\youtube_de_project1-master\\data_Phoenix_migration\\venv\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Join and Transformation Example\")\n",
    "\n",
    "# Create sample RDDs directly from Python lists\n",
    "rdd1 = sc.parallelize([('1', '30'), ('2', '70'), ('3', '90'), ('4', '40')])\n",
    "rdd2 = sc.parallelize([('1', '60'), ('2', '80'), ('3', '100'), ('4', '20')])\n",
    "\n",
    "# Perform the join operation based on the key (the first part of the tuple)\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "\n",
    "# Apply a transformation to filter the results based on a condition\n",
    "# Example: Only keep records where the value in both RDDs are greater than 50\n",
    "filtered_rdd = joined_rdd.filter(lambda x: int(x[1][0]) > 50 and int(x[1][1]) > 50)\n",
    "\n",
    "# Apply another transformation (map) to format the output\n",
    "formatted_rdd = filtered_rdd.map(lambda x: f\"Key: {x[0]}, Value from RDD1: {x[1][0]}, Value from RDD2: {x[1][1]}\")\n",
    "\n",
    "# Collect the results and print them\n",
    "result = formatted_rdd.collect()\n",
    "for item in result:\n",
    "    print(item)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pip apache-spark 1.4.53\\npip robot-framework 3.35.3\\npip pandas 4.43.5.\\n']\n",
      "pip==1.4.53\n",
      "pip\n"
     ]
    }
   ],
   "source": [
    "inpu = '''pip apache-spark 1.4.53\n",
    "pip robot-framework 3.35.3\n",
    "pip pandas 4.43.5.\n",
    "'''\n",
    "\n",
    "lst = inpu.split(\"\\n\")\n",
    "print(lst)\n",
    "\n",
    "for i in lst:\n",
    "    lst1 = i.split(\" \")\n",
    "    # print(lst1)\n",
    "\n",
    "    if len(lst1) >= 2:\n",
    "        print(f\"{lst1[0]}=={lst1[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
